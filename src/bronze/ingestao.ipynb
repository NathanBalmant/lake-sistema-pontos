{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535ab8fe-1724-423a-bf57-dcec26d0155b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "from pyspark.sql import types\n",
    "\n",
    "def tabel_exists(catalog,database, tableName):\n",
    "    \n",
    "    count = (spark.sql(f\"SHOW TABLES FROM {catalog}.{database}\")\n",
    "    .filter(f\"database = '{database}' AND tableName = '{tableName}'\")\n",
    "    .count())\n",
    "\n",
    "    return count == 1\n",
    "\n",
    "def import_schema(table):\n",
    "    # df_full = spark.read.format(\"parquet\").load(f\"/Volumes/raw/{db_schema}/cdc/{table}/\")\n",
    "    with open(f'{table}.json', 'r') as open_file:\n",
    "        schema_json = json.load(open_file)\n",
    "    df_schema = types.StructType.fromJson(schema_json)\n",
    "    return df_schema \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83ddc05-38d0-4af1-9408-87ad6976c810",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "db_schema = 'sistema_pontos'\n",
    "catalog = 'bronze'\n",
    "# table = 'clientes'\n",
    "# table_cdc = 'clientes'\n",
    "# id_field = 'IdClientes'\n",
    "table = dbutils.widgets.get('tablename')\n",
    "table_cdc = dbutils.widgets.get('tablename_cdc') ## tabela que contem o cdc\n",
    "id_field = dbutils.widgets.get('id_field')\n",
    "df_schema = import_schema(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ccc75e-6553-4a25-b7e7-b42c5a2b4b45",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Load"
    }
   },
   "outputs": [],
   "source": [
    "if not tabel_exists(catalog,db_schema,table):\n",
    "\n",
    "  print(\"Tabela nao existente, criando com full_load\")\n",
    "\n",
    "\n",
    "  df_full = spark.read.format(\"parquet\").load(f\"/Volumes/raw/{db_schema}/full_load/{table}/\")\n",
    "  (df_full.coalesce(1)\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .saveAsTable(f\"{catalog}.{db_schema}.{table}\"))\n",
    "\n",
    "else:\n",
    "  print(\"Tabela ja existente, ignorando full_load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19d04ce-449b-42bb-8b12-db5b27795e96",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verificando se o CDC existe"
    }
   },
   "outputs": [],
   "source": [
    "cdc_path = f\"/Volumes/raw/{db_schema}/cdc/{table_cdc}/\"\n",
    "\n",
    "def path_exists(path: str) -> bool:\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "if not path_exists(cdc_path):\n",
    "    print(f\"CDC ainda n√£o existe em {cdc_path}. Pipeline finalizado apenas com FULL LOAD.\")\n",
    "    dbutils.notebook.exit(\"FULL LOAD executado. CDC inexistente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b071d27-ef64-4b57-8d78-79eed98d8654",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Funcao upsert para ser realizada em cada batch"
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{catalog}.{db_schema}.{table}\")\n",
    "\n",
    "def upsert(df, deltaTable):\n",
    "\n",
    "    w = Window.partitionBy(id_field).orderBy(F.col(\"_cdc_ts\").desc())\n",
    "\n",
    "    df_cdc_unique = (df\n",
    "        .withColumn(\"_rn\", F.row_number().over(w))\n",
    "        .filter(F.col(\"_rn\") == 1)\n",
    "        .drop(\"_rn\")\n",
    "    )\n",
    "\n",
    "    (\n",
    "        deltaTable.alias(\"b\")\n",
    "        .merge(df_cdc_unique.alias(\"d\"), f\"b.{id_field} = d.{id_field}\")\n",
    "        .whenMatchedDelete(condition = \"d.op = 'D'\")\n",
    "        .whenMatchedUpdateAll(condition = \"d.op = 'U'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.op = 'I' or d.op = 'U'\")\n",
    "        .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3333958c-344d-4cf0-9bc9-0e87fd11e3ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aplicando CDC para cada batch da stream"
    }
   },
   "outputs": [],
   "source": [
    "df_stream = (spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"parquet\")\n",
    " .schema(df_schema)\n",
    " .load(f\"/Volumes/raw/sistema_pontos/cdc/{table_cdc}/\"))\n",
    "\n",
    "def executar_upsert_no_bronze(df, batchID):\n",
    "    upsert(df, bronze)\n",
    "\n",
    "stream = (df_stream.writeStream.option(\"checkpointLocation\", f\"/Volumes/raw/sistema_pontos/cdc/{table_cdc}_checkpoint/\")\n",
    "           .foreachBatch(executar_upsert_no_bronze))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56c56ce-3ee2-49e2-a15e-4cd2d0c139b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = stream.trigger(availableNow=True).start()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7828523148486679,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
