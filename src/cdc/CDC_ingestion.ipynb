{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6b07c6-f1ae-44cb-8eb8-30d1c0a3bd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "BASE = \"/Volumes/raw/sistema_pontos\"\n",
    "\n",
    "TABLES = [\n",
    "    {\n",
    "        \"name\": \"clientes\",\n",
    "        \"pk\": \"idCliente\",\n",
    "        \"date_field\": \"DtAtualizacao\"   # tem update\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"produtos\",\n",
    "        \"pk\": \"IdProduto\",\n",
    "        \"date_field\": None              #  sem update\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"transacoes\",\n",
    "        \"pk\": \"IdTransacao\",\n",
    "        \"date_field\": None              #  s√≥ DtCriacao\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"transacao_produto\",\n",
    "        \"pk\": \"idTransacaoProduto\",\n",
    "        \"date_field\": None              #  sem update\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833dfb06-b235-47ab-8c17-2742a9e0ac62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_cdc(table_name: str, pk: str, date_field: str | None):\n",
    "\n",
    "    path_baseline = f\"{BASE}/baseline/{table_name}\"\n",
    "    path_actual   = f\"{BASE}/full_load/{table_name}\"\n",
    "    path_cdc      = f\"{BASE}/cdc/{table_name}\"\n",
    "\n",
    "    df_last = spark.read.parquet(path_baseline)\n",
    "    df_act  = spark.read.parquet(path_actual)\n",
    "\n",
    "    # ‚ûï INSERT (pk s√≥ no atual)\n",
    "    inserts = (\n",
    "        df_act.alias(\"a\")\n",
    "        .join(df_last.select(pk).alias(\"l\"), on=pk, how=\"left_anti\")\n",
    "        .select(\"a.*\")\n",
    "        .withColumn(\"op\", F.lit(\"I\"))\n",
    "    )\n",
    "\n",
    "    # ‚ûñ DELETE (pk s√≥ no baseline)\n",
    "    deletes = (\n",
    "        df_last.alias(\"l\")\n",
    "        .join(df_act.select(pk).alias(\"a\"), on=pk, how=\"left_anti\")\n",
    "        .select(\"l.*\")\n",
    "        .withColumn(\"op\", F.lit(\"D\"))\n",
    "    )\n",
    "\n",
    "    frames = [inserts, deletes]\n",
    "\n",
    "    # üîÑ UPDATE (somente se existir date_field)\n",
    "    if date_field:\n",
    "        df_last = df_last.withColumn(\n",
    "            date_field, F.to_timestamp(F.col(date_field))\n",
    "        )\n",
    "        df_act = df_act.withColumn(\n",
    "            date_field, F.to_timestamp(F.col(date_field))\n",
    "        )\n",
    "\n",
    "        updates = (\n",
    "            df_last.alias(\"l\")\n",
    "            .join(df_act.alias(\"a\"), on=pk, how=\"inner\")\n",
    "            .where(F.col(f\"a.{date_field}\") > F.col(f\"l.{date_field}\"))\n",
    "            .select(\"a.*\")\n",
    "            .withColumn(\"op\", F.lit(\"U\"))\n",
    "        )\n",
    "\n",
    "        frames.append(updates)\n",
    "\n",
    "    cdc = (\n",
    "        frames[0]\n",
    "        .unionByName(frames[1])\n",
    "        .unionByName(frames[2]) if len(frames) == 3 else\n",
    "        frames[0].unionByName(frames[1])\n",
    "    ).withColumn(\"_cdc_ts\", F.current_timestamp())\n",
    "\n",
    "    if cdc.limit(1).count() == 0:\n",
    "        print(f\"‚ÑπÔ∏è {table_name}: nenhuma altera√ß√£o\")\n",
    "        return\n",
    "\n",
    "    (\n",
    "        cdc.coalesce(1)\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .format(\"parquet\")\n",
    "        .save(path_cdc)\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ CDC gerado: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70dc4cd5-5183-4377-9393-5f640dd7ce71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for t in TABLES:\n",
    "    create_cdc(\n",
    "        table_name=t[\"name\"],\n",
    "        pk=t[\"pk\"],\n",
    "        date_field=t[\"date_field\"]\n",
    "    )\n",
    "\n",
    "# üîÅ baseline = snapshot atual\n",
    "for t in TABLES:\n",
    "    name = t[\"name\"]\n",
    "    (\n",
    "        spark.read.parquet(f\"{BASE}/full_load/{name}\")\n",
    "        .coalesce(1)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(f\"{BASE}/baseline/{name}\")\n",
    "    )\n",
    "\n",
    "print(\"üîÅ baseline atualizado com sucesso\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CDC_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
